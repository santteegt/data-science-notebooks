{"paragraphs":[{"text":"%md\n# Getting started with Spark Streaming\n\nAuthor [@santteegt](https://santteegt.github.io)\n","user":"anonymous","dateUpdated":"2017-06-21T15:48:09-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Getting started with Spark Streaming</h1>\n<p>Author <a href=\"https://santteegt.github.io\">@santteegt</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1498078048223_1546309382","id":"20170621-154728_955134982","dateCreated":"2017-06-21T15:47:28-0500","dateStarted":"2017-06-21T15:48:09-0500","dateFinished":"2017-06-21T15:48:11-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2323"},{"text":"%md\n## Hwo to work with your own Spark cluster instance\n\n* Start your Spark cluster with at least one master node:\n\n```\nspark_home $ cd sbin \nspark_home $ sh start-master.sh --webui-port 8180\n```\n\n* Open the [http://localhost:8180](Spark WebUI) console and copy the URL property\n* Open the Interpreter configuration from Zeppelin menu options\n* Search the spark interpreter and click on the `edit` button\n* Change the value of the `master` property to the URL copied in above step\n* Save changes\n","user":"anonymous","dateUpdated":"2017-07-05T11:36:51-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Hwo to work with your own Spark cluster instance</h2>\n<ul>\n  <li>Start your Spark cluster with at least one master node:</li>\n</ul>\n<pre><code>spark_home $ cd sbin \nspark_home $ sh start-master.sh --webui-port 8180\n</code></pre>\n<ul>\n  <li>Open the [http://localhost:8180](Spark WebUI) console and copy the URL property</li>\n  <li>Open the Interpreter configuration from Zeppelin menu options</li>\n  <li>Search the spark interpreter and click on the <code>edit</code> button</li>\n  <li>Change the value of the <code>master</code> property to the URL copied in above step</li>\n  <li>Save changes</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1499272270361_-1774742340","id":"20170705-113110_1991662669","dateCreated":"2017-07-05T11:31:10-0500","dateStarted":"2017-07-05T11:36:51-0500","dateFinished":"2017-07-05T11:36:54-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2324"},{"text":"%spark sc","user":"anonymous","dateUpdated":"2017-07-05T11:56:47-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@273e61b7\n"}]},"apps":[],"jobName":"paragraph_1498016878720_-1939783798","id":"20170620-224758_1691438437","dateCreated":"2017-06-20T22:47:58-0500","dateStarted":"2017-07-05T11:56:47-0500","dateFinished":"2017-07-05T11:57:18-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2325"},{"text":"%spark\nsc.version\n","user":"anonymous","dateUpdated":"2017-07-05T11:37:58-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres1: String = 2.1.0\n"}]},"apps":[],"jobName":"paragraph_1499272673701_1738165070","id":"20170705-113753_1732866784","dateCreated":"2017-07-05T11:37:53-0500","dateStarted":"2017-07-05T11:37:58-0500","dateFinished":"2017-07-05T11:37:58-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2326"},{"text":"%md\n## Adding Twitter Streaming API dependency\n\n* Open the Interpreter configuration from Zeppelin menu options\n* Search the spark interpreter and click on the `edit` button\n* At the endof the configuration, add the dependency `org.apache.bahir:spark-streaming-twitter_2.11:2.1.0`. Note that we are using the exact same version of Spark that is printed above. In case that your Spark cluster has a different version, update the dependency accordingly.\n* Save changes\n","user":"anonymous","dateUpdated":"2017-07-05T11:40:52-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Adding Twitter Streaming API dependency</h2>\n<ul>\n  <li>Open the Interpreter configuration from Zeppelin menu options</li>\n  <li>Search the spark interpreter and click on the <code>edit</code> button</li>\n  <li>At the endof the configuration, add the dependency <code>org.apache.bahir:spark-streaming-twitter_2.11:2.1.0</code>. Note that we are using the exact same version of Spark that is printed above. In case that your Spark cluster has a different version, update the dependency accordingly.</li>\n  <li>Save changes</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1499272622354_-531746171","id":"20170705-113702_302087842","dateCreated":"2017-07-05T11:37:02-0500","dateStarted":"2017-07-05T11:40:52-0500","dateFinished":"2017-07-05T11:40:52-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2327"},{"text":"%spark\nimport org.apache.spark._\nimport org.apache.spark.streaming._\n","user":"anonymous","dateUpdated":"2017-06-21T21:50:02-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark._\n\nimport org.apache.spark.streaming._\n"}]},"apps":[],"jobName":"paragraph_1498016893766_-614677938","id":"20170620-224813_1327035508","dateCreated":"2017-06-20T22:48:13-0500","dateStarted":"2017-06-21T21:50:02-0500","dateFinished":"2017-06-21T21:50:03-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2328"},{"text":"%spark\nval batchInterval = 10\nval ssc = new StreamingContext(sc, Seconds(batchInterval))","user":"anonymous","dateUpdated":"2017-06-21T16:03:01-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nbatchInterval: Int = 10\n\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@46a9ed9b\n"}]},"apps":[],"jobName":"paragraph_1498078816679_1647213761","id":"20170621-160016_531238490","dateCreated":"2017-06-21T16:00:16-0500","dateStarted":"2017-06-21T16:03:01-0500","dateFinished":"2017-06-21T16:03:02-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2329"},{"text":"%md\n## Discretized streams (DStream)\n\n- Abstract class in the Spark streaming library, which defines an interface for processing data streams\n- Primary abstraction provided by Spark Streaming for working with data streams\n- Defines the operations that you can perform on a data stream\n\n- Spark Streaming implements a DStream as a sequence of RDDs. Therefore, it inherits the key RDD properties. **It is immutable, partitioned, and fault tolerant**.\n\n- A DStream can be created from a streaming data source or from an existing DStream by applying a transformation\n- Spark Streaming library provides factory methods for creating instances of the concrete classes that implement the interface defined by DStream","user":"anonymous","dateUpdated":"2017-06-21T16:23:03-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Discretized streams (DStream)</h2>\n<ul>\n  <li>Abstract class in the Spark streaming library, which defines an interface for processing data streams</li>\n  <li>Primary abstraction provided by Spark Streaming for working with data streams</li>\n  <li>Defines the operations that you can perform on a data stream</li>\n  <li>\n  <p>Spark Streaming implements a DStream as a sequence of RDDs. Therefore, it inherits the key RDD properties. <strong>It is immutable, partitioned, and fault tolerant</strong>.</p></li>\n  <li>\n  <p>A DStream can be created from a streaming data source or from an existing DStream by applying a transformation</p></li>\n  <li>Spark Streaming library provides factory methods for creating instances of the concrete classes that implement the interface defined by DStream</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1498079787958_2022543676","id":"20170621-161627_1189067279","dateCreated":"2017-06-21T16:16:27-0500","dateStarted":"2017-06-21T16:23:03-0500","dateFinished":"2017-06-21T16:23:03-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2330"},{"text":"%md\n\n### Processing a Data Streams: Transformations & Output operators\n\n```\nval lengths = lines map {line => line.length}\nval words = lines flatMap {line => line.split(\" \")}\nval nonBlankLines = lines filter {line => line.length > 0} // returns a new DStream in which each RDD has the specified number of partitions\ninputStream.repartition(10)\nval combinedStream = stream1.union(stream2)\n// Aggregations\nval countsPerRdd = inputStream.count()\nval longestWords = words reduce { (w1, w2) => if(w1.length > w2.length) w1 else w2}\nval wordCounts = words.countByValue()\nval wordCounts = words.countByValue()\n\n// DStream transformations ONLY for key-value pairs\n\n//  returns a DStream of (K, Seq[V], Seq[W]) when called on a DStream of (K, Seq[V]) and (K, Seq[W]) pairs\nval lines1 = ssc.socketTextStream(\"localhost\", 9999)\nval words1 = lines1 flatMap {line => line.split(\" \")}\nval wordLenPairs1 = words1 map {w => (w.length, w)}\nval wordsByLen1 = wordLenPairs1.groupByKey\nval lines2 = ssc.socketTextStream(\"localhost\", 9998)\nval words2 = lines2 flatMap {line => line.split(\" \")}\nval wordLenPairs2 = words2 map {w => (w.length, w)}\nval wordsByLen2 = wordLenPairs2.groupByKey\nval wordsGroupedByLen = wordsByLen1.cogroup(wordsByLen2)\n\n// Join\nval lines1 = ssc.socketTextStream(\"localhost\", 9999)\nval words1 = lines1 flatMap {line => line.split(\" \")}\nval wordLenPairs1 = words1 map {w => (w.length, w)}\nval lines2 = ssc.socketTextStream(\"localhost\", 9998)\nval words2 = lines2 flatMap {line => line.split(\" \")}\nval wordLenPairs2 = words2 map {w => (w.length, w)}\nval wordsSameLength = wordLenPairs1.join(wordLenPairs2)\n\nval leftOuterJoinDS = wordLenPairs1.leftOuterJoin(wordLenPairs2)\nval rightOuterJoinDS = wordLenPairs1.rightOuterJoin(wordLenPairs2)\nval fullOuterJoinDS = wordLenPairs1.fullOuterJoin(wordLenPairs2)\n\nval wordsByLen = wordLenPairs.groupByKey\nval wordCounts = wordPairs.reduceByKey(_ + _)\n\n```\n","user":"anonymous","dateUpdated":"2017-06-21T16:41:47-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Processing a Data Streams: Transformations &amp; Output operators</h3>\n<pre><code>val lengths = lines map {line =&gt; line.length}\nval words = lines flatMap {line =&gt; line.split(&quot; &quot;)}\nval nonBlankLines = lines filter {line =&gt; line.length &gt; 0} // returns a new DStream in which each RDD has the specified number of partitions\ninputStream.repartition(10)\nval combinedStream = stream1.union(stream2)\n// Aggregations\nval countsPerRdd = inputStream.count()\nval longestWords = words reduce { (w1, w2) =&gt; if(w1.length &gt; w2.length) w1 else w2}\nval wordCounts = words.countByValue()\nval wordCounts = words.countByValue()\n\n// DStream transformations ONLY for key-value pairs\n\n//  returns a DStream of (K, Seq[V], Seq[W]) when called on a DStream of (K, Seq[V]) and (K, Seq[W]) pairs\nval lines1 = ssc.socketTextStream(&quot;localhost&quot;, 9999)\nval words1 = lines1 flatMap {line =&gt; line.split(&quot; &quot;)}\nval wordLenPairs1 = words1 map {w =&gt; (w.length, w)}\nval wordsByLen1 = wordLenPairs1.groupByKey\nval lines2 = ssc.socketTextStream(&quot;localhost&quot;, 9998)\nval words2 = lines2 flatMap {line =&gt; line.split(&quot; &quot;)}\nval wordLenPairs2 = words2 map {w =&gt; (w.length, w)}\nval wordsByLen2 = wordLenPairs2.groupByKey\nval wordsGroupedByLen = wordsByLen1.cogroup(wordsByLen2)\n\n// Join\nval lines1 = ssc.socketTextStream(&quot;localhost&quot;, 9999)\nval words1 = lines1 flatMap {line =&gt; line.split(&quot; &quot;)}\nval wordLenPairs1 = words1 map {w =&gt; (w.length, w)}\nval lines2 = ssc.socketTextStream(&quot;localhost&quot;, 9998)\nval words2 = lines2 flatMap {line =&gt; line.split(&quot; &quot;)}\nval wordLenPairs2 = words2 map {w =&gt; (w.length, w)}\nval wordsSameLength = wordLenPairs1.join(wordLenPairs2)\n\nval leftOuterJoinDS = wordLenPairs1.leftOuterJoin(wordLenPairs2)\nval rightOuterJoinDS = wordLenPairs1.rightOuterJoin(wordLenPairs2)\nval fullOuterJoinDS = wordLenPairs1.fullOuterJoin(wordLenPairs2)\n\nval wordsByLen = wordLenPairs.groupByKey\nval wordCounts = wordPairs.reduceByKey(_ + _)\n\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1498078820003_1747248475","id":"20170621-160020_1443188562","dateCreated":"2017-06-21T16:00:20-0500","dateStarted":"2017-06-21T16:41:47-0500","dateFinished":"2017-06-21T16:41:47-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2331"},{"text":"%md\n### Processing DStreams: special transformations\n\n- **transform** method returns a DStream by applying an RDD => RDD function to each RDD in the source DStream. It takes as argument a function that takes an RDD as argument and returns an RDD. Thus, it gives you direct access to the underlying RDDs of a DStream\n\n**It allows you to use methods provided by the RDD API, but which do not have equivalent operations in the DStream API. For example, sortBy is a transformation not available in the DStream API**\n\n```\nval lines = ssc.socketTextStream(\"localhost\", 9999)\nval words = lines.flatMap{line => line.split(\" \")}\nval sorted = words.transform{rdd => rdd.sortBy((w)=> w)}\n```\n\n**Also useful for applying machine and graph computation algorithms to data streams**.\n\n- **updateStateByKey** method allows you to create and update states for each key in a DStream of key- value pairs. Use this method to maintain any information about each distinct key in a DStream\n\nExample: running count of each distinct word in a DStream\n\n```\n// Set the context to periodically checkpoint the DStream operations for driver fault-tolerance ssc.checkpoint(\"checkpoint\")\nval lines = ssc.socketTextStream(\"localhost\", 9999)\nval words = lines.flatMap{line => line.split(\" \")}\nval wordPairs = words.map{word => (word, 1)}\n// create a function of type (xs: Seq[Int], prevState: Option[Int]) => Option[Int]\nval updateState = (xs: Seq[Int], prevState: Option[Int]) => {\n  prevState match {\n    case Some(prevCount) => Some(prevCount + xs.sum)\n    case None => Some(xs.sum)\n} }\nval runningCount = wordPairs.updateStateByKey(updateState)\n```\n\n\n\n","user":"anonymous","dateUpdated":"2017-06-21T21:51:29-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Processing DStreams: special transformations</h3>\n<ul>\n  <li><strong>transform</strong> method returns a DStream by applying an RDD =&gt; RDD function to each RDD in the source DStream. It takes as argument a function that takes an RDD as argument and returns an RDD. Thus, it gives you direct access to the underlying RDDs of a DStream</li>\n</ul>\n<p><strong>It allows you to use methods provided by the RDD API, but which do not have equivalent operations in the DStream API. For example, sortBy is a transformation not available in the DStream API</strong></p>\n<pre><code>val lines = ssc.socketTextStream(&quot;localhost&quot;, 9999)\nval words = lines.flatMap{line =&gt; line.split(&quot; &quot;)}\nval sorted = words.transform{rdd =&gt; rdd.sortBy((w)=&gt; w)}\n</code></pre>\n<p><strong>Also useful for applying machine and graph computation algorithms to data streams</strong>.</p>\n<ul>\n  <li><strong>updateStateByKey</strong> method allows you to create and update states for each key in a DStream of key- value pairs. Use this method to maintain any information about each distinct key in a DStream</li>\n</ul>\n<p>Example: running count of each distinct word in a DStream</p>\n<pre><code>// Set the context to periodically checkpoint the DStream operations for driver fault-tolerance ssc.checkpoint(&quot;checkpoint&quot;)\nval lines = ssc.socketTextStream(&quot;localhost&quot;, 9999)\nval words = lines.flatMap{line =&gt; line.split(&quot; &quot;)}\nval wordPairs = words.map{word =&gt; (word, 1)}\n// create a function of type (xs: Seq[Int], prevState: Option[Int]) =&gt; Option[Int]\nval updateState = (xs: Seq[Int], prevState: Option[Int]) =&gt; {\n  prevState match {\n    case Some(prevCount) =&gt; Some(prevCount + xs.sum)\n    case None =&gt; Some(xs.sum)\n} }\nval runningCount = wordPairs.updateStateByKey(updateState)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1498079597372_-305680832","id":"20170621-161317_1167735752","dateCreated":"2017-06-21T16:13:17-0500","dateStarted":"2017-06-21T21:51:29-0500","dateFinished":"2017-06-21T21:51:31-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2332"},{"text":"%spark\n","user":"anonymous","dateUpdated":"2017-06-21T16:13:18-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1498079598620_593092599","id":"20170621-161318_2096782469","dateCreated":"2017-06-21T16:13:18-0500","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:2333"},{"text":"%spark\nssc.start()\nssc.awaitTermination()\n","user":"anonymous","dateUpdated":"2017-06-21T17:18:47-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1498079600295_36360940","id":"20170621-161320_627040186","dateCreated":"2017-06-21T16:13:20-0500","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:2334"},{"text":"%md ## A Complete Spark Streaming Application - Twitter\n\nAn application that shows trending Twitter hashtags.\n\n**Important** Prior executing the above code, you must ensure that the artifact `org.apache.spark:spark-streaming-twitter_2.10:1.6.2` is added to the Dependency list in the Spark Intepreter configuration","user":"anonymous","dateUpdated":"2017-06-21T17:03:05-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>A Complete Spark Streaming Application - Twitter</h2>\n<p>An application that shows trending Twitter hashtags.</p>\n<p><strong>Important</strong> Prior executing the above code, you must ensure that the artifact <code>org.apache.spark:spark-streaming-twitter_2.10:1.6.2</code> is added to the Dependency list in the Spark Intepreter configuration</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1498081909139_1944273252","id":"20170621-165149_2022994470","dateCreated":"2017-06-21T16:51:49-0500","dateStarted":"2017-06-21T17:03:05-0500","dateFinished":"2017-06-21T17:03:05-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2335"},{"text":"%spark\nimport org.apache.spark._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.twitter._\nimport twitter4j.Status\n","user":"anonymous","dateUpdated":"2017-06-21T21:53:26-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark._\n\nimport org.apache.spark.streaming._\n\nimport org.apache.spark.streaming.twitter._\n\nimport twitter4j.Status\n"}]},"apps":[],"jobName":"paragraph_1498081930425_-1971518808","id":"20170621-165210_204598058","dateCreated":"2017-06-21T16:52:10-0500","dateStarted":"2017-06-21T21:53:26-0500","dateFinished":"2017-06-21T21:53:29-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2336"},{"text":"%spark\ncase class TwitterAuth(consumerKey: String, consumerSecret: String, accessToken: String, accessTokenSecret: String)\ncase class StreamingConf(lang: String, batchInterval: Long, minThreshold: Int, showCount: Int)\n\nval twitterAuth = TwitterAuth(\"UEr2r5N5qdoBwI0bZT7BXc2bw\", \"jx19jp5JeO5WF2RRfpFuM47ubj3iwXsNRKDIlXVAVDVOCrTiZS\", \"114514416-2Rm8HIvlOXV0v4xIIZnAl0OvTHeNSP6JOAuBgFsI\", \"KiflQsOpWhodp30bWWIb68ZNuAJzMRfaYmr4OkbI0VdsT\")\nval conf = StreamingConf(\"es\", 10, 5, 10)\nval filters = Array(\"mashirafael\", \"correa\", \"glass\")","user":"anonymous","dateUpdated":"2017-06-21T21:53:41-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndefined class TwitterAuth\n\ndefined class StreamingConf\n\ntwitterAuth: TwitterAuth = TwitterAuth(UEr2r5N5qdoBwI0bZT7BXc2bw,jx19jp5JeO5WF2RRfpFuM47ubj3iwXsNRKDIlXVAVDVOCrTiZS,114514416-2Rm8HIvlOXV0v4xIIZnAl0OvTHeNSP6JOAuBgFsI,KiflQsOpWhodp30bWWIb68ZNuAJzMRfaYmr4OkbI0VdsT)\n\nconf: StreamingConf = StreamingConf(es,10,5,10)\n\nfilters: Array[String] = Array(mashirafael, correa, glass)\n"}]},"apps":[],"jobName":"paragraph_1498082057515_1690337333","id":"20170621-165417_1170732229","dateCreated":"2017-06-21T16:54:17-0500","dateStarted":"2017-06-21T21:53:41-0500","dateFinished":"2017-06-21T21:53:45-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2337"},{"text":"%spark\n\nSystem.setProperty(\"twitter4j.oauth.consumerKey\", twitterAuth.consumerKey)\nSystem.setProperty(\"twitter4j.oauth.consumerSecret\", twitterAuth.consumerSecret)\nSystem.setProperty(\"twitter4j.oauth.accessToken\", twitterAuth.accessToken)\nSystem.setProperty(\"twitter4j.oauth.accessTokenSecret\", twitterAuth.accessTokenSecret)\n\nval ssc_twitter = new StreamingContext(sc, Seconds(conf.batchInterval))\nssc_twitter.checkpoint(\"checkpoint\")\n\nval tweets = TwitterUtils.createStream(ssc_twitter, None, filters)\n// val tweetsFilteredByLang = tweets.filter{tweet => tweet.getLang() == conf.lang}\n// val statuses = tweetsFilteredByLang.map{ tweet => tweet.getText()}\n// val words = statuses.flatMap{status => status.split(\"\"\"\\s+\"\"\")}\n// val hashTags = words.filter{word => word.startsWith(\"#\")}\n// val hashTagPairs = hashTags.map{hashtag => (hashtag, 1)}\n// val tagsWithCounts = hashTagPairs.updateStateByKey(\n//                     (counts: Seq[Int], prevCount: Option[Int]) =>\n//                       prevCount.map{c  => c + counts.sum}.orElse{Some(counts.sum)}\n//                   )\n// val topHashTags = tagsWithCounts.filter{ case(t, c) =>\n//                                  c > minThreshold.toInt\n//                               }\n// val sortedTopHashTags = topHashTags.transform{ rdd =>\n//                                   rdd.sortBy({case(w, c) => c}, false)\n//                                  }\n// sortedTopHashTags.print(conf.showCount)\ntweets.print(conf.showCount)\nssc_twitter.start()\nssc_twitter.awaitTermination()","user":"anonymous","dateUpdated":"2017-06-21T21:56:42-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1498082632333_-959785731","id":"20170621-170352_595720476","dateCreated":"2017-06-21T17:03:52-0500","dateStarted":"2017-06-21T21:56:42-0500","dateFinished":"2017-06-21T17:54:53-0500","status":"ABORT","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2338"},{"text":"%spark\nsc.version","user":"anonymous","dateUpdated":"2017-06-21T17:34:13-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres13: String = 2.1.0\n"}]},"apps":[],"jobName":"paragraph_1498084156940_761371767","id":"20170621-172916_144093146","dateCreated":"2017-06-21T17:29:16-0500","dateStarted":"2017-06-21T17:34:13-0500","dateFinished":"2017-06-21T17:34:14-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2339"},{"text":"%spark\nssc.stop(stopSparkContext = false, stopGracefully = true)","user":"anonymous","dateUpdated":"2017-06-21T21:57:57-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1498084453223_210425021","id":"20170621-173413_146850236","dateCreated":"2017-06-21T17:34:13-0500","dateStarted":"2017-06-21T17:56:33-0500","dateFinished":"2017-06-21T17:58:54-0500","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:2340"},{"text":"%spark\n","user":"anonymous","dateUpdated":"2017-06-21T17:55:56-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1498085756094_780304373","id":"20170621-175556_555173822","dateCreated":"2017-06-21T17:55:56-0500","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:2341"}],"name":"Getting started with Spark Streaming","id":"2CN1F93RK","angularObjects":{"2CJ56Y2T2:shared_process":[],"2CHGVB8UT:shared_process":[],"2CKSJZHY7:shared_process":[],"2CMJ576HM:shared_process":[],"2CK5WHF9K:shared_process":[],"2CMRSZMBU:shared_process":[],"2CNE54P8R:shared_process":[],"2CKJPPBB5:shared_process":[],"2CJN36NDV:shared_process":[],"2CMNSJVQ8:shared_process":[],"2CMZMG1BM:shared_process":[],"2CNAUUV5G:shared_process":[],"2CJFE72WK:shared_process":[],"2CMUBKH2V:shared_process":[],"2CJMX11VE:shared_process":[],"2CJF6AAGZ:shared_process":[],"2CMX5TSGV:shared_process":[],"2CNCY4WGQ:shared_process":[],"2CMPUMJXK:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}