{"paragraphs":[{"text":"%md \n# Hello Spark Notebook\n\nAuthor: [@santteegt](https://santteegt.github.io)\n\nNotes are based on the book *Big Data Analytics with Spark: A Practioner's Guide to Using Spark for Large Scale Data Analysis*\n","user":"anonymous","dateUpdated":"2017-06-13T20:34:23-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Hello Spark Notebook</h1>\n<p>Author: <a href=\"https://santteegt.github.io\">@santteegt</a></p>\n<p>Notes are based on the book <em>Big Data Analytics with Spark: A Practioner&rsquo;s Guide to Using Spark for Large Scale Data Analysis</em></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1497243307810_-1207619087","id":"20170611-235507_1771637478","dateCreated":"2017-06-11T23:55:07-0500","dateStarted":"2017-06-13T20:34:23-0500","dateFinished":"2017-06-13T20:34:26-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6836"},{"text":"%spark sc.version\n","user":"anonymous","dateUpdated":"2017-06-13T20:35:38-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres0: String = 2.1.0\n"}]},"apps":[],"jobName":"paragraph_1497243327735_1816304765","id":"20170611-235527_1437256272","dateCreated":"2017-06-11T23:55:27-0500","dateStarted":"2017-06-13T20:35:38-0500","dateFinished":"2017-06-13T20:36:07-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6837"},{"text":"%md\n## Definition\n\nSpark is an in-memory cluster computing framework for processing and analyzing large amounts of data. Spark offers a rich application programming interface (API) for developing big data applications. It also enables you to write more concise code compared to Hadoop MapReduce\n\n### Main Features\n\n* **Fast**: Spark is able to process data hundreds of times faster than Hadoop MapReduce if data fits in memory. Even if data does not fit in memory, Spark can be up to ten times faster than Hadoop MapReduce, as it allows in-memory cluster computing.\n* **Cached data**: Since Spark allows caching of data in memory, the same application implemented with Spark reads data from disk only once. Once data is cached in memory, each subsequent operation can be performed directly on the cached data. **It is up to an application to decide what data should be cached and at what point in a data processing pipeline that data should be cached**\n* **Advanced Execution Engine**: Spark implements an advanced execution engine, which converts a job into a directed acyclic graph (DAG) of stages (just like MapReduce), altough Spark does not force a developer to split a complex data processing algorithm into multiple jobs. **This allows Spark to do optimizations that are not possible with MapReduce**\n* **General Purpose**: Spark can be used for batch processing, interactive analysis, stream processing, machine learning, and graph computing. With Spark, you can use a single framework to build a data processing pipeline that involves different types of data processing tasks. There is no need to learn multiple frameworks or deploy separate clusters for different types of data processing jobs.\n* **Scalable**: You can start with a small cluster, and as your dataset grows, you can add more computing capacity. Thus, Spark allows you to scale economically. **No code change is required when you add a node to a Spark cluster.**\n\n### Ideal Applications\n\n* Iterative Algorithms: algorithms that iterate over the same data multiple times. Applications that use iterative algorithms include machine learning and graph processing applications\n* Interactive Analysis: Spark provides an ideal platform for interactively analyzing a large dataset.\n\n\n","user":"anonymous","dateUpdated":"2017-06-13T20:36:44-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Definition</h2>\n<p>Spark is an in-memory cluster computing framework for processing and analyzing large amounts of data. Spark offers a rich application programming interface (API) for developing big data applications. It also enables you to write more concise code compared to Hadoop MapReduce</p>\n<h3>Main Features</h3>\n<ul>\n  <li><strong>Fast</strong>: Spark is able to process data hundreds of times faster than Hadoop MapReduce if data fits in memory. Even if data does not fit in memory, Spark can be up to ten times faster than Hadoop MapReduce, as it allows in-memory cluster computing.</li>\n  <li><strong>Cached data</strong>: Since Spark allows caching of data in memory, the same application implemented with Spark reads data from disk only once. Once data is cached in memory, each subsequent operation can be performed directly on the cached data. <strong>It is up to an application to decide what data should be cached and at what point in a data processing pipeline that data should be cached</strong></li>\n  <li><strong>Advanced Execution Engine</strong>: Spark implements an advanced execution engine, which converts a job into a directed acyclic graph (DAG) of stages (just like MapReduce), altough Spark does not force a developer to split a complex data processing algorithm into multiple jobs. <strong>This allows Spark to do optimizations that are not possible with MapReduce</strong></li>\n  <li><strong>General Purpose</strong>: Spark can be used for batch processing, interactive analysis, stream processing, machine learning, and graph computing. With Spark, you can use a single framework to build a data processing pipeline that involves different types of data processing tasks. There is no need to learn multiple frameworks or deploy separate clusters for different types of data processing jobs.</li>\n  <li><strong>Scalable</strong>: You can start with a small cluster, and as your dataset grows, you can add more computing capacity. Thus, Spark allows you to scale economically. <strong>No code change is required when you add a node to a Spark cluster.</strong></li>\n</ul>\n<h3>Ideal Applications</h3>\n<ul>\n  <li>Iterative Algorithms: algorithms that iterate over the same data multiple times. Applications that use iterative algorithms include machine learning and graph processing applications</li>\n  <li>Interactive Analysis: Spark provides an ideal platform for interactively analyzing a large dataset.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1497365091577_-656192426","id":"20170613-094451_897773168","dateCreated":"2017-06-13T09:44:51-0500","dateStarted":"2017-06-13T20:36:44-0500","dateFinished":"2017-06-13T20:36:44-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6838"},{"text":"%md\n\n## Higher-lever Architecture\n\nSpark application involves five key entities:\n\n* A **driver program** ->  an application that uses Spark as a library. It can launch one or more jobs in a Spark cluster.\n* A **cluster manager** -> Spark uses it to acquire cluster resources for executing a job. It enables multiple applications to share cluster resources and run on the same worker nodes. Currently supports three cluster managers: standalone, Mesos, and YARN.\n* **Workers** -> worker provides CPU, memory, and storage resources to a Spark application. The workers run a Spark application as distributed processes on a cluster of nodes    \n* **Executors** -> executes application code concurrently in multiple threads. It can also cache data in memory or disk.\n* **Tasks** -> A task is the smallest unit of work that Spark sends to an executor. It is executed by a thread in an executor on a worker node. Spark creates a task per data partition. **The amount of parallelism is determined by the number of partitions**\n\n\n<br/>","user":"anonymous","dateUpdated":"2017-06-13T10:55:56-0500","config":{"colWidth":6,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":390.797,"optionOpen":false}}},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Higher-lever Architecture</h2>\n<p>Spark application involves five key entities:</p>\n<ul>\n  <li>A <strong>driver program</strong> -&gt; an application that uses Spark as a library. It can launch one or more jobs in a Spark cluster.</li>\n  <li>A <strong>cluster manager</strong> -&gt; Spark uses it to acquire cluster resources for executing a job. It enables multiple applications to share cluster resources and run on the same worker nodes. Currently supports three cluster managers: standalone, Mesos, and YARN.</li>\n  <li><strong>Workers</strong> -&gt; worker provides CPU, memory, and storage resources to a Spark application. The workers run a Spark application as distributed processes on a cluster of nodes</li>\n  <li><strong>Executors</strong> -&gt; executes application code concurrently in multiple threads. It can also cache data in memory or disk.</li>\n  <li><strong>Tasks</strong> -&gt; A task is the smallest unit of work that Spark sends to an executor. It is executed by a thread in an executor on a worker node. Spark creates a task per data partition. <strong>The amount of parallelism is determined by the number of partitions</strong></li>\n</ul>\n<br/>\n</div>"}]},"apps":[],"jobName":"paragraph_1497366026869_-906366223","id":"20170613-100026_1161283649","dateCreated":"2017-06-13T10:00:26-0500","dateStarted":"2017-06-13T10:55:56-0500","dateFinished":"2017-06-13T10:55:56-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6839"},{"text":"%md\n\n<img src=\"https://santteegt.github.io/img/blog_images/spark/spark_architecture.png\" alt=\"Spark Architecture\" style=\"width: 450px; heigth: 800px\"/>\n\n\n*Source: Book mentioned in header*","user":"anonymous","dateUpdated":"2017-06-13T10:56:04-0500","config":{"colWidth":6,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<img src=\"https://santteegt.github.io/img/blog_images/spark/spark_architecture.png\" alt=\"Spark Architecture\" style=\"width: 450px; heigth: 800px\"/>\n<p><em>Source: Book mentioned in header</em></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1497367998961_1978427064","id":"20170613-103318_675740571","dateCreated":"2017-06-13T10:33:18-0500","dateStarted":"2017-06-13T10:56:04-0500","dateFinished":"2017-06-13T10:56:04-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6840"},{"text":"%md ## Application Execution\n\n* Spark application (driver program) submits a data processing algorithm as a **job* *(set of computations that Spark performs to return results to a driver program)\n* Spark connects to a cluster manager and acquires executors on the worker nodes\n* Spark splits a job into a directed acyclic graph (DAG) of **stages** (collection of tasks) and then schedules the execution of these stages. **Spark groups tasks into stages using shuffle boundaries**\n* Shuflling data does not randomly redistribute data; it groups data elements into buckets based on some criteria.\n* The executors run the tasks submitted by Spark in parallel\n* **Every Spark application gets its own set of executors on the worker nodes** -> ** Share-nothing architecture\n* ","user":"anonymous","dateUpdated":"2017-06-13T10:44:15-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Application Execution</h2>\n<ul>\n  <li>Spark application (driver program) submits a data processing algorithm as a **job* *(set of computations that Spark performs to return results to a driver program)\n</li>\n  <li>Spark connects to a cluster manager and acquires executors on the worker nodes</li>\n  <li>Spark splits a job into a directed acyclic graph (DAG) of <strong>stages</strong> (collection of tasks) and then schedules the execution of these stages. <strong>Spark groups tasks into stages using shuffle boundaries</strong></li>\n  <li>Shuflling data does not randomly redistribute data; it groups data elements into buckets based on some criteria.</li>\n  <li>The executors run the tasks submitted by Spark in parallel</li>\n  <li><strong>Every Spark application gets its own set of executors on the worker nodes</strong> -&gt; ** Share-nothing architecture<br/>*</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1497366451386_-430586720","id":"20170613-100731_654582122","dateCreated":"2017-06-13T10:07:31-0500","dateStarted":"2017-06-13T10:44:15-0500","dateFinished":"2017-06-13T10:44:15-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6841"},{"text":"%md ## Spark API\n\nAPI consists of two important abstractions: \n\n**SparkContext** ->  main entry point into the Spark library.\n\n```\nval config = new SparkConf().setMaster(\"spark://host:port\").setAppName(\"big app\")\nval sc = new SparkContext(config)\n```\n\n**Resilient Distributed Datasets (RDDs)** -> collection of partitioned data elements that can be operated on in parallel. It represents a distributed dataset and it supports **lazy operations**. RDDs have the following characteristics:\n\n* *Immutable*: an operation that modifies an RDD returns a new RDD.\n* *Partitioned*: Data represented by an RDD is split into partitions.\n* *Fault tolerant*: RDD automatically handles node failures. Spark reconstructs the lost RDD partitions on another node\n* *Interface*: RDD is an interface for processing data. It is defined as an abstract class. Spark provides concrete implementation classes for representing different data sources.\n* *Strongly Typed*\n* *In-Memory*\n\n```\nval xs = (1 to 10000).toList\nval rdd = sc.parallelize(xs)\n\nval rdd = sc.textFile(\"hdfs://namenode:9000/path/to/file-or-directory\")\n```\n","user":"anonymous","dateUpdated":"2017-06-13T10:51:50-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Spark API</h2>\n<p>API consists of two important abstractions: </p>\n<p><strong>SparkContext</strong> -&gt; main entry point into the Spark library.</p>\n<pre><code>val config = new SparkConf().setMaster(&quot;spark://host:port&quot;).setAppName(&quot;big app&quot;)\nval sc = new SparkContext(config)\n</code></pre>\n<p><strong>Resilient Distributed Datasets (RDDs)</strong> -&gt; collection of partitioned data elements that can be operated on in parallel. It represents a distributed dataset and it supports <strong>lazy operations</strong>. RDDs have the following characteristics:</p>\n<ul>\n  <li><em>Immutable</em>: an operation that modifies an RDD returns a new RDD.</li>\n  <li><em>Partitioned</em>: Data represented by an RDD is split into partitions.</li>\n  <li><em>Fault tolerant</em>: RDD automatically handles node failures. Spark reconstructs the lost RDD partitions on another node</li>\n  <li><em>Interface</em>: RDD is an interface for processing data. It is defined as an abstract class. Spark provides concrete implementation classes for representing different data sources.</li>\n  <li><em>Strongly Typed</em></li>\n  <li><em>In-Memory</em></li>\n</ul>\n<pre><code>val xs = (1 to 10000).toList\nval rdd = sc.parallelize(xs)\n\nval rdd = sc.textFile(&quot;hdfs://namenode:9000/path/to/file-or-directory&quot;)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1497368658542_946835877","id":"20170613-104418_309364658","dateCreated":"2017-06-13T10:44:18-0500","dateStarted":"2017-06-13T10:51:50-0500","dateFinished":"2017-06-13T10:51:50-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6842"},{"text":"%md ## RDD Operations\n\nRDD operations can be categorized into two types: \n\n* A **transformation** creates a new RDD. Act as lazy operations.\n* An **action** returns a value to a driver program. It triggers the computation\n","user":"anonymous","dateUpdated":"2017-06-13T11:13:52-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>RDD Operations</h2>\n<p>RDD operations can be categorized into two types: </p>\n<ul>\n  <li>A <strong>transformation</strong> creates a new RDD. Act as lazy operations.</li>\n  <li>An <strong>action</strong> returns a value to a driver program. It triggers the computation</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1497369377300_692561182","id":"20170613-105617_830961686","dateCreated":"2017-06-13T10:56:17-0500","dateStarted":"2017-06-13T11:13:52-0500","dateFinished":"2017-06-13T11:13:52-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6843"},{"text":"%md ### Example of Transformations\n\n```\nval lines = sc.textFile(\"...\")\n\nval lengths = lines map { l => l.length}\nval longLines = lines filter { l => l.length > 80}\nval words = lines flatMap { l => l.split(\" \")}\n\nval lengths = lines mapPartitions { iter => iter.map { l => l.length}}\n\nval linesFile1 = sc.textFile(\"...\")\nval linesFile2 = sc.textFile(\"...\")\n\nval linesFromBothFiles = linesFile1.union(linesFile2)\nval linesPresentInBothFiles = linesFile1.intersection(linesFile2)\nval linesInFile1Only = linesFile1.subtract(linesFile2)\n\nval numbers = sc.parallelize(List(1, 2, 3, 4, 3, 2, 1))\nval uniqueNumbers = numbers.distinct\n\nval numbers = sc.parallelize(List(1, 2, 3, 4))\nval alphabets = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\"))\nval cartesianProduct = numbers.cartesian(alphabets)\n\nval numbers = sc.parallelize(List(1, 2, 3, 4))\nval alphabets = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\"))\nval zippedPairs = numbers.zip(alphabets)\n\nval alphabets = sc.parallelize(List(\"a\", \"b\", \"c\", \"d\"))\nval alphabetsWithIndex = alphabets.zip // zip with index\n```\n","user":"anonymous","dateUpdated":"2017-06-13T22:01:39-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Example of Transformations</h3>\n<pre><code>val lines = sc.textFile(&quot;...&quot;)\n\nval lengths = lines map { l =&gt; l.length}\nval longLines = lines filter { l =&gt; l.length &gt; 80}\nval words = lines flatMap { l =&gt; l.split(&quot; &quot;)}\nval lengths = lines mapPartitions { iter =&gt; iter.map { l =&gt; l.length}}\n\nval linesFile1 = sc.textFile(&quot;...&quot;)\nval linesFile2 = sc.textFile(&quot;...&quot;)\n\nval linesFromBothFiles = linesFile1.union(linesFile2)\nval linesPresentInBothFiles = linesFile1.intersection(linesFile2)\nval linesInFile1Only = linesFile1.subtract(linesFile2)\n\nval numbers = sc.parallelize(List(1, 2, 3, 4, 3, 2, 1))\nval uniqueNumbers = numbers.distinct\n\nval numbers = sc.parallelize(List(1, 2, 3, 4))\nval alphabets = sc.parallelize(List(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;))\nval cartesianProduct = numbers.cartesian(alphabets)\n\nval numbers = sc.parallelize(List(1, 2, 3, 4))\nval alphabets = sc.parallelize(List(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;))\nval zippedPairs = numbers.zip(alphabets)\n\nval alphabets = sc.parallelize(List(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;))\nval alphabetsWithIndex = alphabets.zip // zip with index\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1497369468675_-883480871","id":"20170613-105748_87938493","dateCreated":"2017-06-13T10:57:48-0500","dateStarted":"2017-06-13T11:03:33-0500","dateFinished":"2017-06-13T11:03:33-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6844"},{"text":"%md ### Example of Transformations #2\n\n```\n// GroupBy Example\ncase class Customer(name: String, age: Int, gender: String, zip: String)\nval lines = sc.textFile(\"...\")\nval customers = lines map { l => {\n                val a = l.split(\",\")\n                Customer(a(0), a(1).toInt, a(2), a(3))\n              }\n            }\nval groupByZip = customers.groupBy { c => c.zip}\n\n// KeyBy Example\ncase class Person(name: String, age: Int, gender: String, zip: String)\nval lines = sc.textFile(\"...\")\nval people = lines map { l => {\n                val a = l.split(\",\")\n                Person(a(0), a(1).toInt, a(2), a(3))\n              }\n            }\nval keyedByZip = people.keyBy { p => p.zip}\n\n// SortBy Example\n\nval numbers = sc.parallelize(List(3,2, 4, 1, 5))\nval sorted = numbers.sortBy(x => x, true)\nHere is another example.\ncase class Person(name: String, age: Int, gender: String, zip: String)\nval lines = sc.textFile(\"...\")\nval people = lines map { l => {\n                val a = l.split(\",\")\n                Person(a(0), a(1).toInt, a(2), a(3))\n              }\n            }\nval sortedByAge = people.sortBy( p => p.age, true)\n```\n","user":"anonymous","dateUpdated":"2017-06-13T21:01:03-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Example of Transformations #2</h3>\n<pre><code>// GroupBy Example\ncase class Customer(name: String, age: Int, gender: String, zip: String)\nval lines = sc.textFile(&quot;...&quot;)\nval customers = lines map { l =&gt; {\n                val a = l.split(&quot;,&quot;)\n                Customer(a(0), a(1).toInt, a(2), a(3))\n              }\n            }\nval groupByZip = customers.groupBy { c =&gt; c.zip}\n\n// KeyBy Example\ncase class Person(name: String, age: Int, gender: String, zip: String)\nval lines = sc.textFile(&quot;...&quot;)\nval people = lines map { l =&gt; {\n                val a = l.split(&quot;,&quot;)\n                Person(a(0), a(1).toInt, a(2), a(3))\n              }\n            }\nval keyedByZip = people.keyBy { p =&gt; p.zip}\n\n// SortBy Example\n\nval numbers = sc.parallelize(List(3,2, 4, 1, 5))\nval sorted = numbers.sortBy(x =&gt; x, true)\nHere is another example.\ncase class Person(name: String, age: Int, gender: String, zip: String)\nval lines = sc.textFile(&quot;...&quot;)\nval people = lines map { l =&gt; {\n                val a = l.split(&quot;,&quot;)\n                Person(a(0), a(1).toInt, a(2), a(3))\n              }\n            }\nval sortedByAge = people.sortBy( p =&gt; p.age, true)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1497369680967_-1732133488","id":"20170613-110120_1072291357","dateCreated":"2017-06-13T11:01:20-0500","dateStarted":"2017-06-13T11:03:26-0500","dateFinished":"2017-06-13T11:03:26-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6845"},{"text":"%md ### Example of Transformations #3\n\n```\nval numbers = sc.parallelize((1 to 100).toList)\nval splits = numbers.randomSplit(Array(0.6, 0.2, 0.2))\n\nval numbers = sc.parallelize((1 to 100).toList)\nval numbersWithOnePartition = numbers.coalesce(1) // returns a new RDD with the specified number of partitions.\n\nval numbers = sc.parallelize((1 to 100).toList)\nval numbersWithOnePartition = numbers.repartition(4) // useful for increasing parallelism\n\nval numbers = sc.parallelize((1 to 100).toList)\nval sampleNumbers = numbers.sample(true, 0.2)\n```\n","user":"anonymous","dateUpdated":"2017-06-13T21:06:51-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Example of Transformations #3</h3>\n<pre><code>val numbers = sc.parallelize((1 to 100).toList)\nval splits = numbers.randomSplit(Array(0.6, 0.2, 0.2))\n\nval numbers = sc.parallelize((1 to 100).toList)\nval numbersWithOnePartition = numbers.coalesce(1) // returns a new RDD with the specified number of partitions.\n\nval numbers = sc.parallelize((1 to 100).toList)\nval numbersWithOnePartition = numbers.repartition(4) // useful for increasing parallelism\n\nval numbers = sc.parallelize((1 to 100).toList)\nval sampleNumbers = numbers.sample(true, 0.2)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1497369817409_-1993546557","id":"20170613-110337_286970658","dateCreated":"2017-06-13T11:03:37-0500","dateStarted":"2017-06-13T21:06:51-0500","dateFinished":"2017-06-13T21:06:51-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6846"},{"text":"%md ### Example of Transformations #4\n\n```\nval kvRdd = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"c\", 3)))\n\nval keysRdd = kvRdd.keys\nval valuesRdd = kvRdd.values\nval valuesDoubled = kvRdd mapValues { x => 2*x}\n\nval pairRdd1 = sc.parallelize(List((\"a\", 1), (\"b\",2), (\"c\",3)))\nval pairRdd2 = sc.parallelize(List((\"b\", \"second\"), (\"c\",\"third\"), (\"d\",\"fourth\")))\n\nval joinRdd = pairRdd1.join(pairRdd2)\nval leftOuterJoinRdd = pairRdd1.leftOuterJoin(pairRdd2)\nval rightOuterJoinRdd = pairRdd1.rightOuterJoin(pairRdd2)\nval fullOuterJoinRdd = pairRdd1.fullOuterJoin(pairRdd2)\nval sampleRdd = pairRdd.sampleByKey(true, Map(\"a\"-> 0.1, \"b\"->0.2))\nval resultRdd = pairRdd1.subtractByKey(pairRdd2)\n\nval pairRdd = sc.parallelize(List((\"a\", 1), (\"b\",2), (\"c\",3), (\"a\", 11), (\"b\",22), (\"a\",111)))\nval groupedRdd = pairRdd.groupByKey()\n\nval pairRdd = sc.parallelize(List((\"a\", 1), (\"b\",2), (\"c\",3), (\"a\", 11), (\"b\",22), (\"a\",111)))\nval sumByKeyRdd = pairRdd.reduceByKey((x,y) => x+y)\nval minByKeyRdd = pairRdd.reduceByKey((x,y) => if (x < y) x else y)\n```\n\n","user":"anonymous","dateUpdated":"2017-06-13T11:08:35-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Example of Transformations #4</h3>\n<pre><code>val kvRdd = sc.parallelize(List((&quot;a&quot;, 1), (&quot;b&quot;, 2), (&quot;c&quot;, 3)))\n\nval keysRdd = kvRdd.keys\nval valuesRdd = kvRdd.values\nval valuesDoubled = kvRdd mapValues { x =&gt; 2*x}\n\nval pairRdd1 = sc.parallelize(List((&quot;a&quot;, 1), (&quot;b&quot;,2), (&quot;c&quot;,3)))\nval pairRdd2 = sc.parallelize(List((&quot;b&quot;, &quot;second&quot;), (&quot;c&quot;,&quot;third&quot;), (&quot;d&quot;,&quot;fourth&quot;)))\n\nval joinRdd = pairRdd1.join(pairRdd2)\nval leftOuterJoinRdd = pairRdd1.leftOuterJoin(pairRdd2)\nval rightOuterJoinRdd = pairRdd1.rightOuterJoin(pairRdd2)\nval fullOuterJoinRdd = pairRdd1.fullOuterJoin(pairRdd2)\nval sampleRdd = pairRdd.sampleByKey(true, Map(&quot;a&quot;-&gt; 0.1, &quot;b&quot;-&gt;0.2))\nval resultRdd = pairRdd1.subtractByKey(pairRdd2)\n\nval pairRdd = sc.parallelize(List((&quot;a&quot;, 1), (&quot;b&quot;,2), (&quot;c&quot;,3), (&quot;a&quot;, 11), (&quot;b&quot;,22), (&quot;a&quot;,111)))\nval groupedRdd = pairRdd.groupByKey()\n\nval pairRdd = sc.parallelize(List((&quot;a&quot;, 1), (&quot;b&quot;,2), (&quot;c&quot;,3), (&quot;a&quot;, 11), (&quot;b&quot;,22), (&quot;a&quot;,111)))\nval sumByKeyRdd = pairRdd.reduceByKey((x,y) =&gt; x+y)\nval minByKeyRdd = pairRdd.reduceByKey((x,y) =&gt; if (x &lt; y) x else y)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1497369952380_794141469","id":"20170613-110552_1621773294","dateCreated":"2017-06-13T11:05:52-0500","dateStarted":"2017-06-13T11:08:35-0500","dateFinished":"2017-06-13T11:08:35-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6847"},{"text":"%md ### Examples of Action functions\n\n```\nval rdd = sc.parallelize((1 to 10000).toList)\nval filteredRdd = rdd filter { x => (x % 1000) == 0 }\n\nval filterResult = filteredRdd.collect // returns the values of an RDD as an array\nval total = rdd.count\nval counts = rdd.countByValue\nval firstElement = rdd.first\nval maxElement = rdd.max\nval minElement = rdd.min\nval first3 = rdd.take(3)\nval smallest3 = rdd.takeOrdered(3)\nval largest3 = rdd.top(3)\n\nval numbersRdd = sc.parallelize(List(2, 5, 3, 1))\nval sum = numbersRdd.fold(0) ((partialSum, x) => partialSum + x)\nval product = numbersRdd.fold(1) ((partialProduct, x) => partialProduct * x)\n\nval sum = numbersRdd.reduce ((x, y) => x + y)\nval product = numbersRdd.reduce((x, y) => x * y)\n\nval pairRdd = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"c\", 3), (\"a\", 11), (\"b\", 22), (\"a\", 1)))\n\nval countOfEachKey = pairRdd.countByKey\nval values = pairRdd.lookup(\"a\")\n\nval numbersRdd = sc.parallelize(List(2, 5, 3, 1))\n\n// Numeric actions\n\nval mean = numbersRdd.mean\nval stdev = numbersRdd.stdev\nval sum = numbersRdd.sum\nval variance = numbersRdd.variance\n```\n","user":"anonymous","dateUpdated":"2017-06-13T21:08:43-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Examples of Action functions</h3>\n<pre><code>val rdd = sc.parallelize((1 to 10000).toList)\nval filteredRdd = rdd filter { x =&gt; (x % 1000) == 0 }\n\nval filterResult = filteredRdd.collect // returns the values of an RDD as an array\nval total = rdd.count\nval counts = rdd.countByValue\nval firstElement = rdd.first\nval maxElement = rdd.max\nval minElement = rdd.min\nval first3 = rdd.take(3)\nval smallest3 = rdd.takeOrdered(3)\nval largest3 = rdd.top(3)\n\nval numbersRdd = sc.parallelize(List(2, 5, 3, 1))\nval sum = numbersRdd.fold(0) ((partialSum, x) =&gt; partialSum + x)\nval product = numbersRdd.fold(1) ((partialProduct, x) =&gt; partialProduct * x)\n\nval sum = numbersRdd.reduce ((x, y) =&gt; x + y)\nval product = numbersRdd.reduce((x, y) =&gt; x * y)\n\nval pairRdd = sc.parallelize(List((&quot;a&quot;, 1), (&quot;b&quot;, 2), (&quot;c&quot;, 3), (&quot;a&quot;, 11), (&quot;b&quot;, 22), (&quot;a&quot;, 1)))\n\nval countOfEachKey = pairRdd.countByKey\nval values = pairRdd.lookup(&quot;a&quot;)\n\nval numbersRdd = sc.parallelize(List(2, 5, 3, 1))\n\n// Numeric actions\n\nval mean = numbersRdd.mean\nval stdev = numbersRdd.stdev\nval sum = numbersRdd.sum\nval variance = numbersRdd.variance\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1497370118884_598935860","id":"20170613-110838_1621325292","dateCreated":"2017-06-13T11:08:38-0500","dateStarted":"2017-06-13T11:11:58-0500","dateFinished":"2017-06-13T11:11:58-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6848"},{"text":"%spark\nval rdd = sc.parallelize((1 to 10000).toList)\nval filteredRdd = rdd filter { x => (x % 1000) == 0 }\n\nval filterResult = filteredRdd.collect // returns the values of an RDD as an array\n\nval numbersRdd = sc.parallelize(List(2, 5, 3, 1))\nval sum = numbersRdd.fold(0) ((partialSum, x) => partialSum + x)\nval sum2 = numbersRdd.fold(0) (_ + _)","user":"anonymous","dateUpdated":"2017-06-13T21:13:21-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at parallelize at <console>:27\n\nfilteredRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at filter at <console>:29\n\nfilterResult: Array[Int] = Array(1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000)\n\nnumbersRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at <console>:27\n\nsum: Int = 11\n\nsum2: Int = 11\n"}]},"apps":[],"jobName":"paragraph_1497406132692_-399716646","id":"20170613-210852_1823575902","dateCreated":"2017-06-13T21:08:52-0500","dateStarted":"2017-06-13T21:13:21-0500","dateFinished":"2017-06-13T21:13:26-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6849"},{"text":"%md ### Saving an RDD (Action)\n\n**Since each partition is stored in a separate file, Spark launches multiple tasks and runs them in parallel to write an RDD to a file system**\n\n```\nval numbersRdd = sc.parallelize((1 to 10000).toList)\nval filteredRdd = numbersRdd filter { x => x % 1000 == 0}\nfilteredRdd.saveAsObjectFile(\"numbers-as-object\")\n\nval pairs = (1 to 10000).toList map {x => (x, x*2)}\nval pairsRdd = sc.parallelize(pairs)\nval filteredPairsRdd = pairsRdd filter { case (x, y) => x % 1000 ==0 }\nfilteredPairsRdd.saveAsSequenceFile(\"pairs-as-sequence\")\nfilteredPairsRdd.saveAsTextFile(\"pairs-as-text\")\n\n```\n","user":"anonymous","dateUpdated":"2017-06-13T21:16:08-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Saving an RDD (Action)</h3>\n<p><strong>Since each partition is stored in a separate file, Spark launches multiple tasks and runs them in parallel to write an RDD to a file system</strong></p>\n<pre><code>val numbersRdd = sc.parallelize((1 to 10000).toList)\nval filteredRdd = numbersRdd filter { x =&gt; x % 1000 == 0}\nfilteredRdd.saveAsObjectFile(&quot;numbers-as-object&quot;)\n\nval pairs = (1 to 10000).toList map {x =&gt; (x, x*2)}\nval pairsRdd = sc.parallelize(pairs)\nval filteredPairsRdd = pairsRdd filter { case (x, y) =&gt; x % 1000 ==0 }\nfilteredPairsRdd.saveAsSequenceFile(&quot;pairs-as-sequence&quot;)\nfilteredPairsRdd.saveAsTextFile(&quot;pairs-as-text&quot;)\n\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1497370322614_-928827046","id":"20170613-111202_1971662300","dateCreated":"2017-06-13T11:12:02-0500","dateStarted":"2017-06-13T21:16:08-0500","dateFinished":"2017-06-13T21:16:08-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6850"},{"text":"%md ## Caching RDDs\n\nRDD class provides two methods to cache an RDD: **cache** and **persist**. **RDD caching mechanism is also fault tolerant.**\n\n```\nval logs = sc.textFile(\"path/to/log-files\")\nval errorsAndWarnings = logs filter { l => l.contains(\"ERROR\") || l.contains(\"WARN\")}\nerrorsAndWarnings.cache() // Materializes an RDD data on memory\n\nerrorsAndWarnings.persist(MEMORY_ONLY) // allows an RDD to be stored in memory, disk, or both\n\n```","user":"anonymous","dateUpdated":"2017-06-13T11:17:16-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Caching RDDs</h2>\n<p>RDD class provides two methods to cache an RDD: <strong>cache</strong> and <strong>persist</strong>. <strong>RDD caching mechanism is also fault tolerant.</strong></p>\n<pre><code>val logs = sc.textFile(&quot;path/to/log-files&quot;)\nval errorsAndWarnings = logs filter { l =&gt; l.contains(&quot;ERROR&quot;) || l.contains(&quot;WARN&quot;)}\nerrorsAndWarnings.cache() // Materializes an RDD data on memory\n\nerrorsAndWarnings.persist(MEMORY_ONLY) // allows an RDD to be stored in memory, disk, or both\n\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1497370470150_-662582452","id":"20170613-111430_469209723","dateCreated":"2017-06-13T11:14:30-0500","dateStarted":"2017-06-13T11:17:16-0500","dateFinished":"2017-06-13T11:17:16-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6851"},{"text":"%md\n## Writing the first Spark Application\n\n### Parameters","user":"anonymous","dateUpdated":"2017-06-13T11:18:33-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Writing the first Spark Application</h2>\n<h3>Parameters</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1497328762137_253533080","id":"20170612-233922_1806207848","dateCreated":"2017-06-12T23:39:22-0500","dateStarted":"2017-06-13T11:18:33-0500","dateFinished":"2017-06-13T11:18:33-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6852"},{"text":"%spark\nval inputPath = \"my_data/shakespeare.txt\";\nval outputPath = \"my_data/words_counts\";\n","user":"anonymous","dateUpdated":"2017-06-13T21:24:09-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","tableHide":false,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ninputPath: String = my_data/shakespeare.txt\n\noutputPath: String = my_data/words_counts\n"}]},"apps":[],"jobName":"paragraph_1497243339135_-1328232672","id":"20170611-235539_1030083958","dateCreated":"2017-06-11T23:55:39-0500","dateStarted":"2017-06-13T21:24:09-0500","dateFinished":"2017-06-13T21:24:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6853"},{"text":"%md\n## Words count example","user":"anonymous","dateUpdated":"2017-06-12T23:56:37-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Words count example</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1497329756937_-1720032329","id":"20170612-235556_543774815","dateCreated":"2017-06-12T23:55:56-0500","dateStarted":"2017-06-12T23:56:37-0500","dateFinished":"2017-06-12T23:56:37-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6854"},{"text":"%spark\nval lines = sc.textFile(inputPath)\n","user":"anonymous","dateUpdated":"2017-06-13T21:18:54-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nlines: org.apache.spark.rdd.RDD[String] = my_data/shakespeare.txt MapPartitionsRDD[8] at textFile at <console>:29\n"}]},"apps":[],"jobName":"paragraph_1497328643483_2123627273","id":"20170612-233723_1288340967","dateCreated":"2017-06-12T23:37:23-0500","dateStarted":"2017-06-13T21:18:54-0500","dateFinished":"2017-06-13T21:18:54-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6855"},{"text":"%spark\nval wordCounts = lines.flatMap {line => line.split(\" \")}\n    .map(word => (word, 1))\n    .reduceByKey(_ + _)\n    .repartition(1)\n    .sortBy(tuple => tuple._2, false)\n    \nwordCounts.saveAsTextFile(outputPath)","user":"anonymous","dateUpdated":"2017-06-13T21:58:29-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nwordCounts: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[150] at sortBy at <console>:35\n"}]},"apps":[],"jobName":"paragraph_1497329497309_877055040","id":"20170612-235137_2137721915","dateCreated":"2017-06-12T23:51:37-0500","dateStarted":"2017-06-13T21:58:29-0500","dateFinished":"2017-06-13T21:58:31-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6856"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1497408082106_-1230384064","id":"20170613-214122_1176342064","dateCreated":"2017-06-13T21:41:22-0500","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:6857"}],"name":"HelloSpark - Getting started with the Spark Architecture","id":"2CKTRM8F3","angularObjects":{"2CJ56Y2T2:shared_process":[],"2CHGVB8UT:shared_process":[],"2CKSJZHY7:shared_process":[],"2CMJ576HM:shared_process":[],"2CK5WHF9K:shared_process":[],"2CMRSZMBU:shared_process":[],"2CNE54P8R:shared_process":[],"2CKJPPBB5:shared_process":[],"2CJN36NDV:shared_process":[],"2CMNSJVQ8:shared_process":[],"2CMZMG1BM:shared_process":[],"2CNAUUV5G:shared_process":[],"2CJFE72WK:shared_process":[],"2CMUBKH2V:shared_process":[],"2CJMX11VE:shared_process":[],"2CJF6AAGZ:shared_process":[],"2CMX5TSGV:shared_process":[],"2CNCY4WGQ:shared_process":[],"2CMPUMJXK:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}